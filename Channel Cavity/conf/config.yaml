# Hydra configuration file
# separate "control panel" file where you control running
# how to train, how big batches are, scheduling

# hydra configuration
defaults :
  - physicsnemo_default # start with default settings
  - arch: # override default architecture and with "fully connected"
      - fully_connected
  - scheduler: tf_exponential_lr # set scheduler
  - optimizer: adam # set optimizer
  - loss: sum
  - _self_

# controls learning rate schedule
# every 4000 steps, learning rate is multiplied by 0.95
# this means that early on in training, learning is aggressive
# later on, learning is more conservative (stable convergence)
scheduler:
  decay_rate: 0.95
  decay_steps: 4000

# training control
training:
  rec_validation_freq: 1000 # how often openFOAM validation is evaluated
  rec_inference_freq: 2000 # how often inference plots are produced
  rec_monitor_freq: 1000 # how often losses are logged
  rec_constraint_freq: 2000 # how often constraint wise losses are logged
  max_steps : 10000 # total training iterations

# how many points are sampled per training set per constraint
batch_size:
  NoSlipCavity: 1000
  NoSlipLedge: 100
  Symmetry: 1200
  Inlet: 100
  Outlet: 100
  Interior: 4000

# graph architecture (don't really understand tbh)
graph:
  func_arch: true